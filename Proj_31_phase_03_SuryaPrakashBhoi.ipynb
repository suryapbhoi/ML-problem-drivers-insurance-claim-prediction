{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb756a45",
   "metadata": {},
   "source": [
    "### Project 31 - Porto Seguro’s Safe Driver Prediction - Predict if a driver will file an insurance claim next year.\n",
    "## Phase 3: Modeling and Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c230c65",
   "metadata": {},
   "source": [
    "**In this phase, we will use the below algorithms for our modeling:**\n",
    "1. Naive Bayes Classifier (GaussianNB)\n",
    "2. L1-regularized Logistic Regression\n",
    "3. L2-regularized Logistic Regression\n",
    "4. Elastic Net-regularized Logistic Regression\n",
    "5. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c4ad5",
   "metadata": {},
   "source": [
    "**For model evaluation/ error analysis, we will evaluate/analyse the models using:**\n",
    "1. AUROC\n",
    "2. Log Loss\n",
    "3. Normalized Gini Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511eec3",
   "metadata": {},
   "source": [
    "Import the libraries that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df610af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Supress Future warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# import scikit learn\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "# For train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# Import Support Vector Classifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Cross_validation\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, log_loss\n",
    "\n",
    "# Import pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a939e8f",
   "metadata": {},
   "source": [
    "Load our analytical base (table from previous phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ada9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv('analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031460ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>1</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_calc_02  ps_ind_12_bin  ps_ind_07_bin  ps_ind_15  ps_car_08_cat  \\\n",
       "0         0.5              0              1         11              0   \n",
       "1         0.1              0              0          3              1   \n",
       "2         0.7              0              0         12              1   \n",
       "3         0.9              0              0          8              1   \n",
       "4         0.6              0              0          9              1   \n",
       "\n",
       "   ps_car_02_cat  ps_car_13  ps_car_07_cat  ps_car_04_cat  ps_car_14  \\\n",
       "0              1   0.883679              1              0   0.370810   \n",
       "1              1   0.618817              1              0   0.388716   \n",
       "2              1   0.641586              1              0   0.347275   \n",
       "3              1   0.542949              1              0   0.294958   \n",
       "4              1   0.565832              1              0   0.365103   \n",
       "\n",
       "   ps_ind_02_cat  ps_ind_17_bin  ps_car_11  ps_ind_18_bin  ps_car_12  \\\n",
       "0              2              1          2              0   0.400000   \n",
       "1              1              0          3              1   0.316228   \n",
       "2              4              0          1              0   0.316228   \n",
       "3              1              0          1              0   0.374166   \n",
       "4              2              0          3              0   0.316070   \n",
       "\n",
       "   ps_ind_04_cat  ps_reg_03  ps_reg_01  ps_car_15  ps_calc_01  ps_car_09_cat  \\\n",
       "0              1   0.718070        0.7   3.605551         0.6              0   \n",
       "1              0   0.766078        0.8   2.449490         0.3              2   \n",
       "2              1  -1.000000        0.0   3.316625         0.5              2   \n",
       "3              0   0.580948        0.9   2.000000         0.6              3   \n",
       "4              1   0.840759        0.7   2.000000         0.4              2   \n",
       "\n",
       "   ps_ind_14  ps_ind_16_bin  ps_ind_05_cat  ps_ind_11_bin  ps_ind_01  \\\n",
       "0          0              0              0              0          2   \n",
       "1          0              0              0              0          1   \n",
       "2          0              1              0              0          5   \n",
       "3          0              1              0              0          0   \n",
       "4          0              1              0              0          0   \n",
       "\n",
       "   ps_reg_02  ps_ind_08_bin  ps_car_10_cat  ps_car_03_cat  ps_car_05_cat  \\\n",
       "0        0.2              0              1             -1              1   \n",
       "1        0.4              1              1             -1             -1   \n",
       "2        0.0              1              1             -1             -1   \n",
       "3        0.2              0              1              0              1   \n",
       "4        0.6              0              1             -1             -1   \n",
       "\n",
       "   ps_car_01_cat  ps_car_06_cat  ps_calc_03  ps_ind_10_bin  ps_ind_06_bin  \\\n",
       "0             10              4         0.2              0              0   \n",
       "1             11             11         0.3              0              0   \n",
       "2              7             14         0.1              0              0   \n",
       "3              7             11         0.1              0              1   \n",
       "4             11             14         0.0              0              1   \n",
       "\n",
       "   ps_ind_03  ps_ind_09_bin  target  \n",
       "0          5              0       0  \n",
       "1          7              0       0  \n",
       "2          9              0       0  \n",
       "3          2              0       0  \n",
       "4          0              0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(abt.shape)\n",
    "\n",
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd017f66",
   "metadata": {},
   "source": [
    "Separate the dataframe into separate objects for the imput features(X) and target variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9b6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abt.drop('target', axis=1)\n",
    "y = abt.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7750b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 38)\n",
      "(595212,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cec668",
   "metadata": {},
   "source": [
    "Split X and y into training an test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86e5fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476169 119043 476169 119043\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=abt.target,\n",
    "                                                   random_state=1234)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb406da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    458814\n",
       "1     17355\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d9f234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    114704\n",
       "1      4339\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a327409",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa286dff",
   "metadata": {},
   "source": [
    "**1.Function to calculate Gini coefficient and Normalized Gini Coefficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9302c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Gini coefficient and Normalized Gini Coefficient\n",
    "def gini(actual, pred):\n",
    "    if (len(actual) == len(pred)):\n",
    "        all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n",
    "        all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n",
    "        totalLosses = all[:, 0].sum()\n",
    "        giniSum = all[:, 0].cumsum().sum() / totalLosses\n",
    "\n",
    "        giniSum -= (len(actual) + 1) / 2.\n",
    "        return giniSum / len(actual)\n",
    "    return 0\n",
    "\n",
    "# Function to calculate Normalized Gini Coefficient\n",
    "def gini_normalized(actual, pred):\n",
    "    return gini(actual, pred) / gini(actual, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c8395",
   "metadata": {},
   "source": [
    "**2. Function to fit a RandomizedSearchCV model of specified algorithm and display its cross-validated score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44f64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit a RandomizedSearchCV model of specified algorithm and display its cross-validated score\n",
    "def fit_model_and_display_score(name, X_train, y_train, cv_val):\n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = RandomizedSearchCV(estimator=pipelines[name],\n",
    "                    param_distributions=hyperparameters[name],\n",
    "                    cv=cv_val,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=123,\n",
    "                    verbose=1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted')\n",
    "      \n",
    "    # Best score and params\n",
    "    print(\"Best CV score for \"+ name + \": \") \n",
    "    print( np.round(model.best_score_, 3) )\n",
    "    print(\"Best parameters for \" + name + \": \") \n",
    "    print( model.best_params_ )\n",
    "    \n",
    "    # return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6156e",
   "metadata": {},
   "source": [
    "**3. Function to display Confusion Matrix, TPR and FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c902f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display Confusion Matrix, TPR and FPR\n",
    "def display_model_cm_fpt_tpr(y_test, pred):\n",
    "    # Confusion Matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix (y_test, pred)\n",
    "    print( cm )\n",
    "    print()\n",
    "    # True Positives (TP)\n",
    "    tp = cm[1][1]\n",
    "\n",
    "    # False Positives (FP)\n",
    "    fp = cm[0][1]\n",
    "\n",
    "    # True Negatives (TN)\n",
    "    tn = cm[0][0]\n",
    "\n",
    "    # False Negatives (FN)\n",
    "    fn = cm[1][0]\n",
    "    \n",
    "    #TPR\n",
    "    true_positive_rate = tp/(tp+fn)\n",
    "    print( 'TPR:', np.round(true_positive_rate, 3) )\n",
    "    \n",
    "    #FPR\n",
    "    false_positive_rate = fp/(tn+fp)\n",
    "    print( 'FPR:', np.round(false_positive_rate, 3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0206b04",
   "metadata": {},
   "source": [
    "**4. Function to determine best threshold for classification models**\n",
    "\n",
    "NOTE: The below code for g-means and threshold is partially taken from *https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46614b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine best threshold for classification models\n",
    "def find_best_classification_threshold(y_test, pred_prob):\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_prob)\n",
    "\n",
    "    # calculate the geometric mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    \n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print('Best Threshold=%.3f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "    \n",
    "    #return best threshold\n",
    "    return thresholds[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2745119",
   "metadata": {},
   "source": [
    "**5. Function to save a fitted model to disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "471196c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a fitted model to disk\n",
    "def save_model_to_disk(model, filename):\n",
    "    filename = filename\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model saved to disk as\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa92540",
   "metadata": {},
   "source": [
    "## Build the model Pipeline\n",
    "\n",
    "NOTE: We will add more algorithms to the pipeline as we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "441f5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete the Pipeline dictionary\n",
    "pipelines = {\n",
    "    'nb': make_pipeline(StandardScaler(), GaussianNB()),\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', solver = 'liblinear', random_state=123)),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', solver = 'liblinear', random_state=123)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a61bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451f2da",
   "metadata": {},
   "source": [
    "Create empty dictionary called fitted_models, to include models that have been tuned using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da02df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ca95e",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes (Our Baseline Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a88a8f",
   "metadata": {},
   "source": [
    "Tunable hyperparameters of Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "620d1247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('gaussiannb', GaussianNB(priors=None, var_smoothing=1e-09))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'gaussiannb': GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'gaussiannb__priors': None,\n",
       " 'gaussiannb__var_smoothing': 1e-09}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of our Naive Bayes pipeline\n",
    "pipelines['nb'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e749ad",
   "metadata": {},
   "source": [
    "For Naive Bayes, the impactful hyperparameter is the **var_smoothing**.\n",
    "\n",
    "Declare the parameter grid for **nb (Gaussian Naive Bayes Classifier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ed52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes hyperparameters\n",
    "nb_hyperparameters = {\n",
    "    'gaussiannb__var_smoothing': np.logspace(0,-9, num=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0814f2",
   "metadata": {},
   "source": [
    "Create the hyperparameters dictionary. We will add to this dictionary, as we apply more algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Update hyperparameters dictionary\n",
    "hyperparameters['nb'] = nb_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb99e3e7",
   "metadata": {},
   "source": [
    "**Fit the NaiveBayes model, and save it into fitted_models dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfadf72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "nb has been fitted\n",
      "Best CV score for nb: \n",
      "0.946\n",
      "Best parameters for nb: \n",
      "{'gaussiannb__var_smoothing': 1.0}\n"
     ]
    }
   ],
   "source": [
    "nb_model = fit_model_and_display_score(name='nb', X_train=X_train, y_train=y_train, cv_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d935b",
   "metadata": {},
   "source": [
    "We find that **GaussianNaiveBayes with var_smoothing=1.0**  is our GaussianNB's best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b0ab44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in fitted_models[name] \n",
    "fitted_models['nb'] = nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41aa4bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk as nb_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Save the Gaussian Naive Bayes model to disk\n",
    "save_model_to_disk(nb_model, 'nb_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2a893",
   "metadata": {},
   "source": [
    "Get the predicted classes from our Gaussian Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "274ed5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using our fitted Gaussian Naive Bayes model\n",
    "nb_pred = fitted_models['nb'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7e518",
   "metadata": {},
   "source": [
    "Check the Confusion Matrix, TPR and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "875f8160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[112354   2350]\n",
      " [  4184    155]]\n",
      "\n",
      "TPR: 0.036\n",
      "FPR: 0.02\n"
     ]
    }
   ],
   "source": [
    "display_model_cm_fpt_tpr(y_test, nb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6510f",
   "metadata": {},
   "source": [
    "**Lets check the AUROC, Log-Loss and Gini Coefficient:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61853111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using fitted gaussian Naive Bayes\n",
    "nb_pred_prob = fitted_models['nb'].predict_proba(X_test)\n",
    "\n",
    "# Get JUST the PREDICTION PROBABILITY for positive class\n",
    "nb_pred_prob = [ p[1] for p in nb_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64243f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Naive Bayes model:\n",
      "AUROC:  0.613\n",
      "Log Loss:  0.279\n",
      "Normalized ini Coefficient:  0.019\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC\n",
    "print(\"For Naive Bayes model:\")\n",
    "print( \"AUROC: \",np.round(roc_auc_score(y_test, nb_pred_prob),3) )\n",
    "\n",
    "# Calculate Log-Loss\n",
    "log_loss_nb = np.round(log_loss(y_test, nb_pred_prob), 3)\n",
    "print(\"Log Loss: \", log_loss_nb)\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "print(\"Normalized ini Coefficient: \", np.round(gini_normalized(y_test, nb_pred),3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbf038",
   "metadata": {},
   "source": [
    "**Summary of the Naive Bayes model:**\n",
    "1. We get cross-validated score of 0.946\n",
    "2. TPR = 0.036, FPR = 0.02\n",
    "3. AUROC = 0.613\n",
    "4. Log Loss = 0.279\n",
    "5. Normalized Gini Coefficient = 0.019\n",
    "\n",
    "Let's consider this as our baseline scores, and go ahead with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6dc9e",
   "metadata": {},
   "source": [
    "### 2. L1-regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2973e107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                      random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                      warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                    random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'logisticregression__C': 1.0,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__l1_ratio': None,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'auto',\n",
       " 'logisticregression__n_jobs': None,\n",
       " 'logisticregression__penalty': 'l1',\n",
       " 'logisticregression__random_state': 123,\n",
       " 'logisticregression__solver': 'liblinear',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of Logistic Regression pipeline\n",
    "pipelines['l1'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7242b",
   "metadata": {},
   "source": [
    "For **regularized logistic regression**, the most impactful hyperparameter is the **strength of the penalty.**\n",
    "\n",
    "Declare the hyperparameter grids for l1 ( 𝐿1 -regularized logistic regression) as values between 0.001 and 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0e84b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-regularized Logistic Regression hyperparameters\n",
    "l1_hyperparameters = {\n",
    "    'logisticregression__C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "febc71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the existing hyperparameters dictionary\n",
    "hyperparameters['l1'] = l1_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ab2ca",
   "metadata": {},
   "source": [
    "#### Fit the L1-Logistic regression model, and save it into fitted_models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c7957d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "l1 has been fitted\n",
      "Best CV score for l1: \n",
      "0.964\n",
      "Best parameters for l1: \n",
      "{'logisticregression__C': 5}\n"
     ]
    }
   ],
   "source": [
    "l1_model = fit_model_and_display_score(name='l1', X_train=X_train, y_train=y_train, cv_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d3a98",
   "metadata": {},
   "source": [
    "We find that **Logistic Regression with C=5**  is our L1-Logistic Regression's best estimator with **cross-validated score of 0.964**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f92c86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in fitted_models[name] \n",
    "fitted_models['l1'] = l1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ed206bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk as l1_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Save the L1 Logistic Regression model to disk\n",
    "save_model_to_disk(l1_model, 'l1_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdedb01",
   "metadata": {},
   "source": [
    "Get the **predicted classes** from our L1-regularized Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3e4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using our fitted L1-regularized Logistic Regression model\n",
    "l1_pred = fitted_models['l1'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18672e93",
   "metadata": {},
   "source": [
    "Check the **Confusion Matrix, TPR and FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ceac795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[114704      0]\n",
      " [  4339      0]]\n",
      "\n",
      "TPR: 0.0\n",
      "FPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "display_model_cm_fpt_tpr(y_test, l1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9c5aa",
   "metadata": {},
   "source": [
    "**TPR and FPR values are 'zero'**, i.e., **the fitted L1-regularized Logistic Regression predicts only negative class**  with the default threshold=0.5.\n",
    "\n",
    "Lets find the best threshold for L1 Logistic regression model (using G-Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94cc0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using fitted L1-Logistic Regression\n",
    "l1_pred_prob = fitted_models['l1'].predict_proba(X_test)\n",
    "\n",
    "# Get JUST the PREDICTION PROBABILITY for positive class\n",
    "l1_pred_prob = [ p[1] for p in l1_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61af1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.035, G-Mean=0.585\n"
     ]
    }
   ],
   "source": [
    "# Best threshold for L1 Logistic Regression model\n",
    "l1_threshold = np.round( find_best_classification_threshold(y_test, l1_pred_prob), 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c3b1a",
   "metadata": {},
   "source": [
    "Get the updated predictions with the **best threshold=0.035**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ae1d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[66964 47740]\n",
      " [ 1797  2542]]\n",
      "\n",
      "TPR: 0.586\n",
      "FPR: 0.416\n"
     ]
    }
   ],
   "source": [
    "l1_pred = (fitted_models['l1'].predict_proba(X_test)[:,1] >= 0.035).astype(bool)\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n",
    "display_model_cm_fpt_tpr(y_test, l1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc603a2",
   "metadata": {},
   "source": [
    "Lets check the **AUROC, Log Loss and Gini Coefficient**  of the L1-regularized Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f4c2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For L1-regularized Logistic Regression model:\n",
      "AUROC:  0.617\n",
      "Log Loss:  0.153\n",
      "Normalized Gini Coefficient:  0.172\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC\n",
    "print(\"For L1-regularized Logistic Regression model:\")\n",
    "print( \"AUROC: \",np.round(roc_auc_score(y_test, l1_pred_prob),3) )\n",
    "\n",
    "# Calculate Log-Loss\n",
    "log_loss_l1 = np.round(log_loss(y_test, l1_pred_prob), 3)\n",
    "print(\"Log Loss: \", log_loss_l1)\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "print(\"Normalized Gini Coefficient: \", np.round(gini_normalized(y_test, l1_pred),3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459e967",
   "metadata": {},
   "source": [
    "**Summary of the L1-regularized Logistic regression model:**\n",
    "1. We get cross-validated score of 0.964\n",
    "2. True Positive rate = 0.586, False Positive Rate = 0.416\n",
    "3. We get AUROC of 0.617 (Slight improvement as compared to our Naive Bayes model)\n",
    "4. Log-Loss of 0.153 (Better than our Naive Bayes model)\n",
    "5. Normalized Gini Coefficient = 0.172 (Much improved than Naive Bayes model)\n",
    "\n",
    "So, we see improved scores as compared to Gaussian Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67aa18",
   "metadata": {},
   "source": [
    "### 3. L2-regularized Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c621c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-regularized Logistic Regression hyperparameters\n",
    "l2_hyperparameters = {\n",
    "    'logisticregression__C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80318ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the existing hyperparameters dictionary\n",
    "hyperparameters['l2'] = l2_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c0561",
   "metadata": {},
   "source": [
    "#### Fit the L2-Logistic regression model, and save it into fitted_models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98432fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "l2 has been fitted\n",
      "Best CV score for l2: \n",
      "0.964\n",
      "Best parameters for l2: \n",
      "{'logisticregression__C': 5}\n"
     ]
    }
   ],
   "source": [
    "l2_model = fit_model_and_display_score(name='l2', X_train=X_train, y_train=y_train, cv_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd565e",
   "metadata": {},
   "source": [
    "We find that **Logistic Regression with C=5**  is our L2-Logistic Regression's best estimator with **cross-validated score of 0.964**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "653f2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in fitted_models[name] \n",
    "fitted_models['l2'] = l2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8551a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk as l2_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Save the L2 Logistic Regression model to disk\n",
    "save_model_to_disk(l2_model, 'l2_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8059a23",
   "metadata": {},
   "source": [
    "Get the **predicted classes** from our L2-regularized Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8a25568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using our fitted L2-regularized Logistic Regression model\n",
    "l2_pred = fitted_models['l2'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f1266",
   "metadata": {},
   "source": [
    "Check the **Confusion Matrix, TPR and FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1a8b8bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[114704      0]\n",
      " [  4339      0]]\n",
      "\n",
      "TPR: 0.0\n",
      "FPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "display_model_cm_fpt_tpr(y_test, l2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44791c0a",
   "metadata": {},
   "source": [
    "**TPR and FPR values are 'zero'**, i.e., **the fitted L2-regularized Logistic Regression predicts only negative class**  with the default threshold=0.5.\n",
    "\n",
    "Lets find the best threshold for L2 Logistic regression model (using G-Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6bfd9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using fitted L2-Logistic Regression\n",
    "l2_pred_prob = fitted_models['l2'].predict_proba(X_test)\n",
    "\n",
    "# Get JUST the PREDICTION PROBABILITY for positive class\n",
    "l2_pred_prob = [ p[1] for p in l2_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b435eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.035, G-Mean=0.585\n"
     ]
    }
   ],
   "source": [
    "# Best threshold for L2 Logistic Regression model\n",
    "l2_threshold = np.round( find_best_classification_threshold(y_test, l2_pred_prob), 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab7cd7",
   "metadata": {},
   "source": [
    "Get the updated predictions with the **best threshold=0.035**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcfb0817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[66963 47741]\n",
      " [ 1797  2542]]\n",
      "\n",
      "TPR: 0.586\n",
      "FPR: 0.416\n"
     ]
    }
   ],
   "source": [
    "l2_pred = (fitted_models['l2'].predict_proba(X_test)[:,1] >= 0.035).astype(bool)\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n",
    "display_model_cm_fpt_tpr(y_test, l2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9f3ac",
   "metadata": {},
   "source": [
    "Lets check the **AUROC, Log Loss and Gini Coefficient**  of the L2-regularized Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66662107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For L2-regularized Logistic Regression model:\n",
      "AUROC:  0.617\n",
      "Log Loss:  0.153\n",
      "Normalized Gini Coefficient:  0.172\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC\n",
    "print(\"For L2-regularized Logistic Regression model:\")\n",
    "print( \"AUROC: \",np.round(roc_auc_score(y_test, l2_pred_prob),3) )\n",
    "\n",
    "# Calculate Log-Loss\n",
    "log_loss_l2 = np.round(log_loss(y_test, l2_pred_prob), 3)\n",
    "print(\"Log Loss: \", log_loss_l2)\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "print(\"Normalized Gini Coefficient: \", np.round(gini_normalized(y_test, l2_pred),3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfde9d",
   "metadata": {},
   "source": [
    "**Summary of the L2-regularized Logistic regression model:**\n",
    "1. We get cross-validated score of 0.964 (same as L1-Logistic Regression)\n",
    "2. True Positive Rate = 0.586, False Positive Rate = 0.416 (same as L1-Logistic Regression)\n",
    "3. We get AUROC of 0.617 (same as L1-Logistic Regression)\n",
    "4. Log loss of 0.153 (same as L1-Logistic Regression)\n",
    "5. Normalized Gini Coefficient = 0.172 (same as L1-Logistic Regression)\n",
    "\n",
    "So, we see that the performance of our L2_regularized Logistic Regression model is very similar to our L1-regularized Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf588c7",
   "metadata": {},
   "source": [
    "### 4. ElasticNet-regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "084b5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the Pipeline dictionary\n",
    "pipelines['enet'] = make_pipeline(StandardScaler(), \n",
    "                                  LogisticRegression(penalty='elasticnet', solver = 'saga', random_state=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daab7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net - Logistic Regression hyperparameters\n",
    "enet_hyperparameters = {\n",
    "    'logisticregression__C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000],\n",
    "    'logisticregression__l1_ratio': [.1, .5, .7,.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecd9289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the existing hyperparameters dictionary\n",
    "hyperparameters['enet'] = enet_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a48db",
   "metadata": {},
   "source": [
    "**Fit the ElasticNet-Logistic regression model, and save it into fitted_models dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f3ff9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "enet has been fitted\n",
      "Best CV score for enet: \n",
      "0.964\n",
      "Best parameters for enet: \n",
      "{'logisticregression__l1_ratio': 0.1, 'logisticregression__C': 1000}\n"
     ]
    }
   ],
   "source": [
    "enet_model = fit_model_and_display_score(name='enet', X_train=X_train, y_train=y_train, cv_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec18d68",
   "metadata": {},
   "source": [
    "We find that **Elastic Net Logistic Regression with C=1000 and L1_ratio=0.1**  is our Elastic Net-Logistic Regression's best estimator with **cross-validated score=0.964**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37f030a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in fitted_models[name] \n",
    "fitted_models['enet'] = enet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfd42831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk as enet_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Save the Elastic Net Logistic Regression model to disk\n",
    "save_model_to_disk(enet_model, 'enet_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ae40",
   "metadata": {},
   "source": [
    "Get the **predicted classes** from our ElasticNet-regularized Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c742c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using our fitted ElasticNet-regularized Logistic Regression model\n",
    "enet_pred = fitted_models['enet'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3673da",
   "metadata": {},
   "source": [
    "Check the **Confusion Matrix, TPR and FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4147617e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[114704      0]\n",
      " [  4339      0]]\n",
      "\n",
      "TPR: 0.0\n",
      "FPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "display_model_cm_fpt_tpr(y_test, enet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd77f2b",
   "metadata": {},
   "source": [
    "**TPR and FPR values are 'zero'**, i.e., **the fitted Elastic Net-regularized Logistic Regression predicts only negative class**  with the default threshold=0.5.\n",
    "\n",
    "Lets find the best threshold for Elastic Net Logistic regression model (using G-Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e329c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using fitted L2-Logistic Regression\n",
    "enet_pred_prob = fitted_models['enet'].predict_proba(X_test)\n",
    "\n",
    "# Get JUST the PREDICTION PROBABILITY for positive class\n",
    "enet_pred_prob = [ p[1] for p in enet_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca6641b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.035, G-Mean=0.585\n"
     ]
    }
   ],
   "source": [
    "# Best threshold for Elastic Net Logistic Regression model\n",
    "enet_threshold = np.round( find_best_classification_threshold(y_test, enet_pred_prob), 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22960fed",
   "metadata": {},
   "source": [
    "Get the updated predictions with the **best threshold=0.035**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f877486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[66964 47740]\n",
      " [ 1797  2542]]\n",
      "\n",
      "TPR: 0.586\n",
      "FPR: 0.416\n"
     ]
    }
   ],
   "source": [
    "enet_pred = (fitted_models['enet'].predict_proba(X_test)[:,1] >= 0.035).astype(bool)\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n",
    "display_model_cm_fpt_tpr(y_test, enet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b6f3e",
   "metadata": {},
   "source": [
    "Lets check the **AUROC, Log Loss and Gini Coefficient**  of the Elastic Net-regularized Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41a1c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Elastic Net-regularized Logistic Regression model:\n",
      "AUROC:  0.617\n",
      "Log Loss:  0.153\n",
      "Normalized Gini Coefficient:  0.172\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC\n",
    "print(\"For Elastic Net-regularized Logistic Regression model:\")\n",
    "print( \"AUROC: \",np.round(roc_auc_score(y_test, enet_pred_prob),3) )\n",
    "\n",
    "# Calculate Log-Loss\n",
    "log_loss_enet = np.round(log_loss(y_test, enet_pred_prob), 3)\n",
    "print(\"Log Loss: \", log_loss_enet)\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "print(\"Normalized Gini Coefficient: \", np.round(gini_normalized(y_test, enet_pred),3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8aba3",
   "metadata": {},
   "source": [
    "**Summary of the Elastic Net-regularized Logistic regression model:**\n",
    "1. We get cross-validated score of 0.964 (same as L2-Logistic Regression)\n",
    "2. True Positive Rate = 0.586, False Positive Rate = 0.416 (same as L1-Logistic Regression and L2-Logistic Regression)\n",
    "3. AUROC of 0.617 (same as L1 and L2-Logistic Regression)\n",
    "4. Log Loss of 0.153 (same as L1 and L2-Logistic Regression)\n",
    "5. Normalized Gini Coefficient = 0.172 (same as L1 and L2-Logistic Regression)\n",
    "\n",
    "Hence, we conclude that **all the Logistic Regression models (L1, L2 and Elastic Net) perform the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac6e39",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28c947b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the Pipeline dictionary\n",
    "pipelines['dt'] = make_pipeline(StandardScaler(), \n",
    "                                  DecisionTreeClassifier(splitter='best', random_state=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53ddd279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('decisiontreeclassifier',\n",
       "   DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_samples_leaf=1,\n",
       "                          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                          random_state=123, splitter='best'))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'decisiontreeclassifier': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        random_state=123, splitter='best'),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'decisiontreeclassifier__ccp_alpha': 0.0,\n",
       " 'decisiontreeclassifier__class_weight': None,\n",
       " 'decisiontreeclassifier__criterion': 'gini',\n",
       " 'decisiontreeclassifier__max_depth': None,\n",
       " 'decisiontreeclassifier__max_features': None,\n",
       " 'decisiontreeclassifier__max_leaf_nodes': None,\n",
       " 'decisiontreeclassifier__min_impurity_decrease': 0.0,\n",
       " 'decisiontreeclassifier__min_samples_leaf': 1,\n",
       " 'decisiontreeclassifier__min_samples_split': 2,\n",
       " 'decisiontreeclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'decisiontreeclassifier__random_state': 123,\n",
       " 'decisiontreeclassifier__splitter': 'best'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of our SGD Classifier pipeline\n",
    "pipelines['dt'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "981fad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree Classifier hyperparameters\n",
    "dt_hyperparameters = {\n",
    "    \"decisiontreeclassifier__max_depth\": [2,4,6,8,10],\n",
    "    \"decisiontreeclassifier__max_features\": [5, 10],\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": [10, 20, 30],\n",
    "    \"decisiontreeclassifier__criterion\": [\"gini\", \"entropy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb31840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the existing hyperparameters dictionary\n",
    "hyperparameters['dt'] = dt_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ceba8d",
   "metadata": {},
   "source": [
    "**Fit the DecisionTree Classification model, and save it into fitted_models dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b800d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "dt has been fitted\n",
      "Best CV score for dt: \n",
      "0.964\n",
      "Best parameters for dt: \n",
      "{'decisiontreeclassifier__min_samples_leaf': 10, 'decisiontreeclassifier__max_features': 10, 'decisiontreeclassifier__max_depth': 6, 'decisiontreeclassifier__criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "dt_model = fit_model_and_display_score(name='dt', X_train=X_train, y_train=y_train, cv_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418637b4",
   "metadata": {},
   "source": [
    "We find that **DecisionTree Classifier with min_samples_leaf= 10, max_features = 10, max_depth= 6 and criterion= 'gini'**  is our DecisionTree Classifier's best estimator with **cross-validated score=0.964**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbf74a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in fitted_models[name] \n",
    "fitted_models['dt'] = dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c127df0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk as dt_model.sav\n"
     ]
    }
   ],
   "source": [
    "# Save the Decision Tree Classifier model to disk\n",
    "save_model_to_disk(dt_model, 'dt_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6331ca7",
   "metadata": {},
   "source": [
    "Get the **predicted classes** from our ElasticNet-regularized Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f14f6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using our fitted Decision Tree Classifier model\n",
    "dt_pred = fitted_models['dt'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9d017",
   "metadata": {},
   "source": [
    "Check the **Confusion Matrix, TPR and FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90ddd1c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[114701      3]\n",
      " [  4338      1]]\n",
      "\n",
      "TPR: 0.0\n",
      "FPR: 0.0\n"
     ]
    }
   ],
   "source": [
    "display_model_cm_fpt_tpr(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994d0f1",
   "metadata": {},
   "source": [
    "**TPR and FPR values are 'zero'**  with the default threshold=0.5.\n",
    "\n",
    "Lets find the best threshold for Elastic Net Logistic regression model (using G-Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8565f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using fitted DecisionTree Classifier\n",
    "dt_pred_prob = fitted_models['dt'].predict_proba(X_test)\n",
    "\n",
    "# Get JUST the PREDICTION PROBABILITY for positive class\n",
    "dt_pred_prob = [ p[1] for p in dt_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "966a7cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.037, G-Mean=0.567\n"
     ]
    }
   ],
   "source": [
    "# Best threshold for DecisionTree Classifier model\n",
    "dt_threshold = np.round( find_best_classification_threshold(y_test, dt_pred_prob), 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50da01",
   "metadata": {},
   "source": [
    "Get the updated predictions with the **best threshold=0.037**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d687633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[67574 47130]\n",
      " [ 1976  2363]]\n",
      "\n",
      "TPR: 0.545\n",
      "FPR: 0.411\n"
     ]
    }
   ],
   "source": [
    "dt_pred = (fitted_models['dt'].predict_proba(X_test)[:,1] >= 0.037).astype(bool)\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n",
    "display_model_cm_fpt_tpr(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41220b18",
   "metadata": {},
   "source": [
    "Lets check the **AUROC, Log Loss and Gini Coefficient**  of the Elastic Net-regularized Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a422ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree Classifier model:\n",
      "AUROC:  0.595\n",
      "Log Loss:  0.155\n",
      "Normalized Gini Coefficient:  0.136\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUROC\n",
    "print(\"For Decision Tree Classifier model:\")\n",
    "print( \"AUROC: \",np.round(roc_auc_score(y_test, dt_pred_prob),3) )\n",
    "\n",
    "# Calculate Log-Loss\n",
    "log_loss_dt = np.round(log_loss(y_test, dt_pred_prob), 3)\n",
    "print(\"Log Loss: \", log_loss_dt)\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "print(\"Normalized Gini Coefficient: \", np.round(gini_normalized(y_test, dt_pred),3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f907417",
   "metadata": {},
   "source": [
    "**Summary of the Decision Tree Classifier model:**\n",
    "1. We get cross-validated score of 0.964 (same as Logistic Regression)\n",
    "2. True Positive Rate = 0.545, False Positive Rate = 0.411 (slightly less than the Logistic Regression models)\n",
    "3. AUROC of 0.595 (less than the Logistic Regression models)\n",
    "4. Log Loss of 0.155 (Logistic Regression models are slightly better than this)\n",
    "5. Normalized Gini Coefficient = 0.136 (less than the Logistic Regression models)\n",
    "\n",
    "Hence, we conclude that the **Decision Tree Classifier's evaluation metrics are slightly poorer than the Logistic Regression models (L1, L2 and Elastic Net)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f32310",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af37438",
   "metadata": {},
   "source": [
    "As part of model building, we built the below models:\n",
    "1. Naive Bayes Classifier (GaussianNB)\n",
    "2. L1-regularized Logistic Regression\n",
    "3. L2-regularized Logistic Regression\n",
    "4. Elastic Net-regularized Logistic Regression\n",
    "5. Decision Tree Classifier\n",
    "\n",
    "As part of model evaluation and error analysis, we evaluated/anaylzed their:\n",
    "1. AUROC\n",
    "2. Log Loss\n",
    "3. Normalized Gini Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c48ac",
   "metadata": {},
   "source": [
    "We found that all the **Logistic Regression models (L1, L2 and Elastic-Net) performed the best** and had the same metric values:\n",
    "* AUROC of 0.617\n",
    "* Log Loss of 0.153\n",
    "* Normalized Gini Coefficient = 0.172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e4faa",
   "metadata": {},
   "source": [
    "**NOTE:** In next phase, we will cover more advanced modeling techniques like Random Forests, Gradient Boosted Trees, XGBoost Classifier, Multi Layer Perceptron.\n",
    "We will try to match the Kaggle topper's leaderboard Normalized Gini Coefficient score of **0.297**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
